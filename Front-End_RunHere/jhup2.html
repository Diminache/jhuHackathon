<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Page 2</title>
    <link href='https://fonts.googleapis.com/css?family=Inclusive Sans' rel='stylesheet'>
    <link href='https://fonts.googleapis.com/css?family=Fredoka' rel='stylesheet'>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Simulated conversation with a physician (Not Functional, ran out of free API credit)</h1>
        </div>
    </header>

    <br>

    <main>
        <section class="voice-input">
            <a href="index.html" class="page-button animate-color" padding="5px;">Home</a>
            <a href="jhup3.html" class="page-button animate-color" style="margin-right: -10%;">Med info summarizer</a>
            <br>
            <br>
            <h2>Click Mic to speak, press again to stop recording!</h2>
            <h6>The text will update as you speak, once you are done, scroll to the bottom for transcription.</h6>
            
            <br>
            <!-- Use a div with class "microphone-container" to create the blue circle -->
            
            <div class="microphone-container" id="toggle-microphone" style="cursor: pointer;">
                <div class="microphone recording">üéôÔ∏è</div>
            </div>
            <div id="ongoing-transcription" class="voice-input">
                <!-- Transcription text will xxxappear here -->
                <div id="chat-log"></div>
            </div>
            <br>
        </section>

        <section class="summary">
        
            <div class="iframe-container">
                <iframe class="iFrameTag"
                        allow="microphone;"
                        src="https://console.dialogflow.com/api-client/demo/embedded/dde5a4f2-d403-4ca3-9293-00dedffe4b2c">
                </iframe>
              </div>
              
        </section>
    </main>

    <div class="creators">
        <p>&copy; 2023 Daanish Bawan, Tahir Ismailwala, Waleed Raza</p>
    </div>

    <!-- JavaScript and other scripts can be included here -->


    <script>

const socket = new WebSocket("ws://35.221.31.91:5001"); // Replace with your WebSocket server URL

        // Function to handle messages received from the server
        socket.onmessage = (event) => {
            console.log(`Received: ${event.data}`);
            const FinalTranscriptUpdated = document.getElementById("ongoing-transcription")
            FinalTranscriptUpdated.textContent = event.data;
        };

        // Function to handle errors
        socket.onerror = (error) => {
            console.error(`WebSocket Error: ${error.message}`);
        };

        // Function to handle connection open event
        socket.onopen = (event) => {
            console.log("Connected to server.");

            // Example: Send a message to the server after the connection is open
            sendMessage("Hello, WebSocket server!");
        };

        // Function to handle connection close event
        socket.onclose = (event) => {
            if (event.wasClean) {
                console.log(`Connection closed cleanly, code=${event.code}, reason=${event.reason}`);
            } else {
                console.error("Connection died");
            }
        };

        // Function to send a message to the server
        function sendMessage(message) {
            socket.send(message);
        }












        const chatLogContainer = document.getElementById('chat-log');

        // Get references to HTML elements
        const toggleMicrophone = document.getElementById('toggle-microphone');
        const transcriptionTextarea = document.getElementById('ongoing-transcription');
        const saveButton = document.getElementById('save-button'); // A button to save the transcript
    
        // Initialize variables for speech recognition
        let recognition = null; // Speech recognition object
        let isRecording = false; // Flag to track whether recording is active
        let finalTranscript = ''; // Store the final transcript
        let interimTranscript = ''; // Store interim results
    
        // Initialize the speech recognition when the DOM is loaded
        window.addEventListener('DOMContentLoaded', () => {
            recognition = new webkitSpeechRecognition();
            recognition.continuous = true; // Continuous recognition
            recognition.interimResults = true; // Get interim results during recognition
    
            // Handle results from the speech recognition
            recognition.onresult = (event) => {
                interimTranscript = ''; // Clear interim results
                for (let i = event.resultIndex; i < event.results.length; i++) {
                    if (event.results[i].isFinal) {
                        finalTranscript += event.results[i][0].transcript; // Store final results
                    } else {
                        interimTranscript += event.results[i][0].transcript; // Store interim results
                    }
                }
                updateTranscription();
            };
    
            // Handle errors that may occur during recognition
            recognition.onerror = (event) => {
                console.error(event.error);
            };
        });
    
        // Add a click event listener to the microphone button
        toggleMicrophone.addEventListener('click', () => {
            // Check if speech recognition is supported by the browser
            if (!recognition.start) {
                alert('Speech recognition is not supported by your browser. Please use a compatible browser.');
                return;
            }

            // Start or stop recording based on the current state
            if (!isRecording) {
                // Do not clear transcripts when starting a new recording session
                transcriptionTextarea.textContent = 'Listening...';
                recognition.start(); // Start speech recognition
                toggleMicrophone.textContent = 'üéôÔ∏è';
                toggleMicrophone.classList.add('recording');
                isRecording = true; // Set recording state to true
            } else {
                recognition.stop(); // Stop speech recognition
                toggleMicrophone.textContent = 'üéôÔ∏è';
                toggleMicrophone.classList.remove('recording');
                isRecording = false; // Set recording state to false
                saveButton.disabled = false; // Enable the save button once recording stops

                RequestLaymansTerms(finalTranscript); // Requests Layman's terms
            }
        });

    
        // Add a click event listener to the save button
        saveButton.addEventListener('click', () => {
            // You can now access the finalTranscript variable and do whatever you want with it
            alert('Final Transcript: ' + finalTranscript);
        });
    
        // Initialize an array to store the chat log
        let chatLog = [];
    
        <!-- Inside your updateTranscription() function -->
        // Function to update the transcription textarea and chat log
        function updateTranscription() {
            const newText = interimTranscript.trim() + ' ' + finalTranscript.trim();
            transcriptionTextarea.textContent = newText;
        
            // Split the newText into separate lines
            const lines = newText.split('\n');
        
            // Append each line as a separate element in the chat log
            for (const line of lines) {
                // Create a new <p> element for the user input
                const userInputElement = document.createElement('p');
                userInputElement.textContent = line;
        
                // Append the <p> element to the chat log container
                chatLogContainer.appendChild(userInputElement);
            }
        }

        // Function to request layman's terms using finalTranscript
        function RequestLaymansTerms(transcript) {
            // Implement the logic to request layman's terms here
            // You can use the 'transcript' variable to send the user's input for processing
    
            // For example, you can send a request to a server for layman's terms
            // using an API or perform any other necessary actions here.
        }
    
        const socket = new WebSocket('ws://127.0.0.1:5001'); // Change the URL to your WebSocket server
    
        socket.addEventListener('open', (event) => {
            console.log('Connected to WebSocket server');
    
            // Send data to the server
            const sendData = "Albert Einstein is smart";
            socket.send(sendData);
        });
        // Function to add user input to the chat log
        function addUserInputToChatLog(userInput) {
            // Create a new <p> element for the user input
            const userInputElement = document.createElement('p');
            userInputElement.textContent = userInput;

            // Append the <p> element to the chat log container
            chatLogContainer.appendChild(userInputElement);
        }

        socket.addEventListener('message', (event) => {
            console.log('Message received from server:', event.data);
        });
    
        socket.addEventListener('close', (event) => {
            if (event.wasClean) {
                console.log('Connection closed cleanly, code:', event.code, 'reason:', event.reason);
            } else {
                console.error('Connection abruptly closed');
            }
        });
    
        socket.addEventListener('error', (error) => {
            console.error('WebSocket error:', error);
        });
        addUserInputToChatLog(interimTranscript.trim() + ' ' + finalTranscript.trim());

        </script>


    </script>
    


</body>
</html>
